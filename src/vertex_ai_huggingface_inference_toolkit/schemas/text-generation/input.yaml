title: TextGenerationPipelineInput
type: object
required:
- text_inputs
properties:
  text_inputs:
    oneOf:
      - type: string
      - type: array
        items:
          type: string
    description: One or several prompts (or one list of prompts) to complete.
  return_tensors:
    type: boolean
    default: false
    description: Whether or not to return the tensors of predictions (as token indices) in the outputs. If set to True, the decoded text is not returned.
  return_text:
    type: boolean
    default: true
    description: >
      Whether or not to return the decoded texts in the outputs.
  return_full_text:
    type: boolean
    default: true
    description: >
      If set to False only added text is returned, otherwise the full text is returned. Only meaningful if return_text is set to True.
  clean_up_tokenization_spaces:
    type: boolean
    default: True
    description: Whether or not to clean up the potential extra spaces in the text output.
  prefix:
    type: string
    default: null
    description: Prefix added to prompt.
  handle_long_generation:
    type: string
    default: null
    description: >
      By default, this pipelines does not handle long generation (ones that exceed in one form or the other the model maximum length). There is no perfect way to adress this (more info :https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227). This provides common strategies to work around that problem depending on your use case.
      None : default strategy where nothing in particular happens
      "hole": Truncates left of input, and leaves a gap wide enough to let generation happen (might truncate a lot of the prompt and not suitable when generation exceed the model capacity)
  generate_kwargs:
    type: boolean
    default: null
    description: >
      Additional keyword arguments to pass along to the generate method of the model (see the generate method corresponding to your framework here).

